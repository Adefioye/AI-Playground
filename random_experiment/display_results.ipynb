{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89455a2a",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba80b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a83c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json, math, re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Mapping, Optional, Sequence, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "PRIMARY_METRICS = [\"acc\", \"acc_norm\", \"exact_match\", \"word_perplexity\"]\n",
    "EXTRA_METRICS   = [\"perplexity\", \"byte_perplexity\", \"bits_per_byte\"]\n",
    "ALL_METRICS     = PRIMARY_METRICS + EXTRA_METRICS  # column order base\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Loading / discovery\n",
    "# ----------------------------\n",
    "def load_json(path: str | Path) -> Dict[str, Any]:\n",
    "    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def find_model_pairs(input_dir: str | Path) -> Dict[str, Dict[str, Path]]:\n",
    "    input_dir = Path(input_dir)\n",
    "    pat = re.compile(r\"^(?P<model>.+?)_(?P<run>[12])\\.json$\", flags=re.IGNORECASE)\n",
    "    pairs: Dict[str, Dict[str, Path]] = {}\n",
    "    for p in input_dir.glob(\"*.json\"):\n",
    "        m = pat.match(p.name)\n",
    "        if not m: \n",
    "            continue\n",
    "        pairs.setdefault(m.group(\"model\"), {})[m.group(\"run\")] = p\n",
    "    return {m: d for m, d in pairs.items() if \"1\" in d and \"2\" in d}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Parse & consolidate (two runs)\n",
    "# ----------------------------\n",
    "def _canonicalize_metric(raw_metric: str) -> str:\n",
    "    if \",\" in raw_metric:\n",
    "        base, *quals = raw_metric.split(\",\")\n",
    "        qual = \"__\".join(q.strip() for q in quals if q.strip())\n",
    "        return f\"{base.strip()}__{qual}\" if qual else base.strip()\n",
    "    return raw_metric.strip()\n",
    "\n",
    "def _keep_metric(metric_name: str) -> bool:\n",
    "    base = metric_name.split(\"__\", 1)[0]\n",
    "    return (base in ALL_METRICS) or (base.endswith(\"_stderr\") and base[:-7] in ALL_METRICS)\n",
    "\n",
    "def parse_wandb_summary(summary: Mapping[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for key, value in summary.items():\n",
    "        if \"/\" not in key:\n",
    "            continue\n",
    "        task, raw_metric = key.split(\"/\", 1)\n",
    "        if raw_metric == \"alias\" or raw_metric.endswith(\"_eval_results\"):\n",
    "            continue\n",
    "        metric = _canonicalize_metric(raw_metric)\n",
    "        if not _keep_metric(metric):\n",
    "            continue\n",
    "        keep = None\n",
    "        if isinstance(value, (int, float)) or value is None:\n",
    "            keep = value\n",
    "        else:\n",
    "            try:\n",
    "                num = float(value)\n",
    "                if math.isfinite(num):\n",
    "                    keep = num\n",
    "            except Exception:\n",
    "                pass\n",
    "        if keep is not None:\n",
    "            out.setdefault(task, {})[metric] = keep\n",
    "    return out\n",
    "\n",
    "def consolidate_two_summaries(\n",
    "    summary1: Mapping[str, Any],\n",
    "    summary2: Mapping[str, Any],\n",
    "    labels: Tuple[str, str] = (\"run_1\", \"run_2\"),\n",
    ") -> Dict[str, Any]:\n",
    "    s1 = parse_wandb_summary(summary1)\n",
    "    s2 = parse_wandb_summary(summary2)\n",
    "    agg: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "    for task, metrics in s1.items():\n",
    "        t = agg.setdefault(task, {})\n",
    "        for m, v in metrics.items():\n",
    "            t.setdefault(m, {})[labels[0]] = v\n",
    "    for task, metrics in s2.items():\n",
    "        t = agg.setdefault(task, {})\n",
    "        for m, v in metrics.items():\n",
    "            t.setdefault(m, {})[labels[1]] = v\n",
    "    return {\"metadata\": {\"runs\": [{\"label\": labels[0]}, {\"label\": labels[1]}]}, \"tasks\": agg}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Build “metrics-first” table\n",
    "#   Rows: tasks\n",
    "#   Columns: metric per run (e.g., acc[run_1], acc[run_2], acc_norm[run_1], …)\n",
    "#   Missing -> \"–\"\n",
    "# ----------------------------\n",
    "def build_metrics_first_table(consolidated: Mapping[str, Any]) -> pd.DataFrame:\n",
    "    runs = [r[\"label\"] for r in consolidated.get(\"metadata\", {}).get(\"runs\", [])]\n",
    "    tasks = consolidated.get(\"tasks\", {})\n",
    "\n",
    "    # Gather metric names actually present (plus stderr variants)\n",
    "    present_metrics = set()\n",
    "    for m_map in tasks.values():\n",
    "        present_metrics.update(m_map.keys())\n",
    "\n",
    "    # Expand desired column order: main metrics first, then any others we kept (deterministic)\n",
    "    ordered_metrics = []\n",
    "    for m in ALL_METRICS:\n",
    "        # include base and any qualified variants (e.g., exact_match__strict-match)\n",
    "        variants = sorted([x for x in present_metrics if x == m or x.startswith(m + \"__\")])\n",
    "        ordered_metrics.extend(variants if variants else [m])  # keep slot even if mostly missing\n",
    "    # add any remaining metrics (stderr etc.) not in ALL_METRICS\n",
    "    extras = sorted([m for m in present_metrics if m.split(\"__\",1)[0] not in ALL_METRICS])\n",
    "    ordered_metrics.extend(extras)\n",
    "\n",
    "    # Build rows\n",
    "    rows = {}\n",
    "    for task, m_map in tasks.items():\n",
    "        row = {}\n",
    "        for metric in ordered_metrics:\n",
    "            for run in runs:\n",
    "                key = f\"{metric} [{run}]\"\n",
    "                val = m_map.get(metric, {}).get(run, None)\n",
    "                row[key] = val\n",
    "        rows[task] = row\n",
    "\n",
    "    df = pd.DataFrame.from_dict(rows, orient=\"index\")\n",
    "    # Replace missing with en dash for readability\n",
    "    df = df.where(pd.notna(df), \"–\")\n",
    "    # Stable column order\n",
    "    df = df[[col for col in rows[next(iter(rows))].keys()]] if rows else df\n",
    "    df.index.name = \"task\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Render/save: HTML + Excel\n",
    "# ----------------------------\n",
    "def _fmt_cell(val: Any, metric_name: str) -> str:\n",
    "    base = metric_name.split(\"__\", 1)[0]\n",
    "    if val == \"–\":\n",
    "        return \"–\"\n",
    "    try:\n",
    "        v = float(val)\n",
    "        if base in {\"acc\", \"acc_norm\", \"exact_match\"} and 0 <= v <= 1:\n",
    "            return f\"{v:.1%}\"\n",
    "        if base in {\"word_perplexity\",\"perplexity\",\"byte_perplexity\",\"bits_per_byte\"}:\n",
    "            return f\"{v:.3f}\"\n",
    "        return f\"{v:.4f}\"\n",
    "    except Exception:\n",
    "        return str(val)\n",
    "\n",
    "def render_html_table(df: pd.DataFrame, out_html: str | Path, title: str = \"LM Harness Metrics\") -> Path:\n",
    "    out_html = Path(out_html)\n",
    "    out_html.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build a per-column formatter using the metric name extracted from header\n",
    "    formatters = {}\n",
    "    for col in df.columns:\n",
    "        metric_name = col.split(\" [\", 1)[0]\n",
    "        formatters[col] = (lambda mn: (lambda v: _fmt_cell(v, mn)))(metric_name)\n",
    "\n",
    "    styler = (\n",
    "        df.style\n",
    "          .format(formatters)\n",
    "          .set_caption(title)\n",
    "          .set_table_styles([\n",
    "              {\"selector\": \"caption\", \"props\": [(\"caption-side\",\"top\"), (\"font-size\",\"16px\"), (\"font-weight\",\"600\"), (\"margin-bottom\",\"8px\")]},\n",
    "              {\"selector\": \"th\", \"props\": [(\"position\",\"sticky\"), (\"top\",\"0\"), (\"background\",\"#fafafa\"), (\"z-index\",\"2\")]},\n",
    "              {\"selector\": \"th.row_heading\", \"props\": [(\"position\",\"sticky\"), (\"left\",\"0\"), (\"background\",\"#fafafa\"), (\"z-index\",\"3\")]},\n",
    "              {\"selector\": \"td, th\", \"props\": [(\"border\",\"1px solid #ddd\"), (\"padding\",\"6px 8px\"), (\"font-size\",\"12px\"), (\"white-space\",\"nowrap\")]},\n",
    "          ])\n",
    "          .set_properties(**{\"text-align\": \"right\"})\n",
    "    )\n",
    "    # left-align the task index\n",
    "    styler = styler.set_properties(subset=pd.IndexSlice[:, :], **{}).set_table_styles([\n",
    "        {\"selector\": \"tbody th\", \"props\": [(\"text-align\",\"left\")]},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    html = styler.to_html()\n",
    "    out_html.write_text(html, encoding=\"utf-8\")\n",
    "    return out_html\n",
    "\n",
    "# def save_excel_table(df: pd.DataFrame, out_xlsx: str | Path, sheet_name: str = \"metrics\") -> Path:\n",
    "#     out_xlsx = Path(out_xlsx)\n",
    "#     out_xlsx.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
    "#         df.to_excel(writer, sheet_name=sheet_name)\n",
    "#         wb  = writer.book\n",
    "#         ws  = writer.sheets[sheet_name]\n",
    "\n",
    "#         # Formats\n",
    "#         header_fmt = wb.add_format({\"bold\": True, \"bg_color\": \"#EFEFEF\", \"border\":1, \"text_wrap\": False})\n",
    "#         text_fmt   = wb.add_format({\"border\":1})\n",
    "#         num_fmt    = wb.add_format({\"border\":1})\n",
    "#         ws.freeze_panes(1, 1)   # freeze header row and task column\n",
    "#         ws.autofilter(0, 0, df.shape[0], df.shape[1])  # enable filters\n",
    "\n",
    "#         # Column widths heuristic\n",
    "#         ws.set_column(0, 0, 24, text_fmt)  # task index\n",
    "#         for j, col in enumerate(df.columns, start=1):\n",
    "#             width = max(10, min(28, int(max( len(str(col)), df[col].astype(str).str.len().quantile(0.9) ) + 2 )))\n",
    "#             ws.set_column(j, j, width, num_fmt)\n",
    "\n",
    "#         # Header styling\n",
    "#         for j in range(df.shape[1] + 1):\n",
    "#             ws.write(0, j, ws.table[0][j] if hasattr(ws, \"table\") else ws.cell(0, j), header_fmt)\n",
    "\n",
    "#     return out_xlsx\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# End-to-end per-model driver\n",
    "# ----------------------------\n",
    "def render_per_model_tables(\n",
    "    input_dir: str | Path,\n",
    "    output_root: str | Path,\n",
    "    *,\n",
    "    table_stem: str = \"task_metrics\",\n",
    "    run_labels: Tuple[str, str] = (\"run_1\", \"run_2\"),\n",
    ") -> Dict[str, Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    For each <model> with <model>_1.json and <model>_2.json:\n",
    "      - consolidate\n",
    "      - build metrics-first table (rows=tasks; columns=every metric per run)\n",
    "      - save HTML and XLSX (Excel) in outputs/<model>/\n",
    "\n",
    "    Returns: { model: {\"html\": Path, \"xlsx\": Path} }\n",
    "    \"\"\"\n",
    "    models = find_model_pairs(input_dir)\n",
    "    output_root = Path(output_root)\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results: Dict[str, Dict[str, Path]] = {}\n",
    "    if not models:\n",
    "        print(f\"[INFO] No complete model pairs found in {input_dir}.\")\n",
    "        return results\n",
    "\n",
    "    print(f\"[INFO] Found {len(models)} model(s).\")\n",
    "    for model, pair in sorted(models.items()):\n",
    "        print(f\"\\n[MODEL] {model}\")\n",
    "        s1, s2 = load_json(pair[\"1\"]), load_json(pair[\"2\"])\n",
    "        consolidated = consolidate_two_summaries(s1, s2, labels=run_labels)\n",
    "        df = build_metrics_first_table(consolidated)\n",
    "\n",
    "        out_dir = output_root / model\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        html_path = out_dir / f\"{table_stem}.html\"\n",
    "        xlsx_path = out_dir / f\"{table_stem}.xlsx\"\n",
    "\n",
    "        render_html_table(df, html_path, title=f\"{model} — LM Harness Metrics\")\n",
    "\n",
    "        print(f\"  - HTML: {html_path}\")\n",
    "        results[model] = {\"html\": html_path}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb726b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1 model(s).\n",
      "\n",
      "[MODEL] mha\n",
      "  - HTML: outputs/mha/lmh_metrics.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mha': {'html': PosixPath('outputs/mha/lmh_metrics.html')}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = render_per_model_tables(\n",
    "    input_dir=\"./results\",   # where <model>_1.json & <model>_2.json live\n",
    "    output_root=\"./outputs\",     # will create per-model folders\n",
    "    table_stem=\"lmh_metrics\",\n",
    "    run_labels=(\"run_1\",\"run_2\"),\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3986712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (2.3.0)\n",
      "Collecting xlsxwriter\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ai-playground/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8f9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
