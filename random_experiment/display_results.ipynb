{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89455a2a",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c8f9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt2</th>\n",
       "      <th>mha</th>\n",
       "      <th>mla192-96-0</th>\n",
       "      <th>mla192-96-192</th>\n",
       "      <th>mla0-96-192</th>\n",
       "      <th>mla0-0-192</th>\n",
       "      <th>mla0-128-0</th>\n",
       "      <th>mla192-0-0</th>\n",
       "      <th>mla0-0-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hellaswag (acc)</th>\n",
       "      <td>28.92</td>\n",
       "      <td>27.11</td>\n",
       "      <td>27.18</td>\n",
       "      <td>27.01</td>\n",
       "      <td>27.21</td>\n",
       "      <td>27.14</td>\n",
       "      <td>27.05</td>\n",
       "      <td>27.14</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambada_openai (acc)</th>\n",
       "      <td>32.56</td>\n",
       "      <td>17.25</td>\n",
       "      <td>13.06</td>\n",
       "      <td>11.08</td>\n",
       "      <td>11.62</td>\n",
       "      <td>15.02</td>\n",
       "      <td>13.89</td>\n",
       "      <td>15.82</td>\n",
       "      <td>17.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmlu (acc)</th>\n",
       "      <td>22.92</td>\n",
       "      <td>22.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>22.94</td>\n",
       "      <td>22.92</td>\n",
       "      <td>22.92</td>\n",
       "      <td>22.94</td>\n",
       "      <td>22.95</td>\n",
       "      <td>22.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>piqa (acc)</th>\n",
       "      <td>62.89</td>\n",
       "      <td>61.32</td>\n",
       "      <td>61.43</td>\n",
       "      <td>59.63</td>\n",
       "      <td>60.01</td>\n",
       "      <td>61.37</td>\n",
       "      <td>60.50</td>\n",
       "      <td>61.21</td>\n",
       "      <td>61.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikitext (word_perplexity)</th>\n",
       "      <td>37.37</td>\n",
       "      <td>80.63</td>\n",
       "      <td>89.50</td>\n",
       "      <td>96.61</td>\n",
       "      <td>92.87</td>\n",
       "      <td>82.44</td>\n",
       "      <td>85.11</td>\n",
       "      <td>81.84</td>\n",
       "      <td>80.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winogrande (acc)</th>\n",
       "      <td>51.62</td>\n",
       "      <td>53.28</td>\n",
       "      <td>50.75</td>\n",
       "      <td>51.38</td>\n",
       "      <td>52.57</td>\n",
       "      <td>51.46</td>\n",
       "      <td>50.59</td>\n",
       "      <td>49.96</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commonsense_qa (acc)</th>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.49</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "      <td>19.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmedqa (acc)</th>\n",
       "      <td>45.40</td>\n",
       "      <td>38.20</td>\n",
       "      <td>38.60</td>\n",
       "      <td>36.60</td>\n",
       "      <td>34.40</td>\n",
       "      <td>39.20</td>\n",
       "      <td>35.00</td>\n",
       "      <td>37.60</td>\n",
       "      <td>38.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race (acc)</th>\n",
       "      <td>29.47</td>\n",
       "      <td>25.36</td>\n",
       "      <td>23.83</td>\n",
       "      <td>24.40</td>\n",
       "      <td>24.11</td>\n",
       "      <td>24.02</td>\n",
       "      <td>25.74</td>\n",
       "      <td>26.32</td>\n",
       "      <td>25.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sciq (acc)</th>\n",
       "      <td>75.20</td>\n",
       "      <td>56.20</td>\n",
       "      <td>56.90</td>\n",
       "      <td>54.10</td>\n",
       "      <td>51.90</td>\n",
       "      <td>57.50</td>\n",
       "      <td>55.30</td>\n",
       "      <td>57.90</td>\n",
       "      <td>56.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc273 (acc)</th>\n",
       "      <td>58.61</td>\n",
       "      <td>53.11</td>\n",
       "      <td>51.65</td>\n",
       "      <td>52.38</td>\n",
       "      <td>54.95</td>\n",
       "      <td>49.82</td>\n",
       "      <td>51.65</td>\n",
       "      <td>49.45</td>\n",
       "      <td>53.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xnli (acc)</th>\n",
       "      <td>35.01</td>\n",
       "      <td>34.32</td>\n",
       "      <td>34.22</td>\n",
       "      <td>34.10</td>\n",
       "      <td>34.67</td>\n",
       "      <td>34.47</td>\n",
       "      <td>34.70</td>\n",
       "      <td>34.43</td>\n",
       "      <td>34.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            gpt2   mha  mla192-96-0  mla192-96-192  \\\n",
       "hellaswag (acc)            28.92 27.11        27.18          27.01   \n",
       "lambada_openai (acc)       32.56 17.25        13.06          11.08   \n",
       "mmlu (acc)                 22.92 22.95        22.95          22.94   \n",
       "piqa (acc)                 62.89 61.32        61.43          59.63   \n",
       "wikitext (word_perplexity) 37.37 80.63        89.50          96.61   \n",
       "winogrande (acc)           51.62 53.28        50.75          51.38   \n",
       "commonsense_qa (acc)       19.57 19.57        19.57          19.49   \n",
       "pubmedqa (acc)             45.40 38.20        38.60          36.60   \n",
       "race (acc)                 29.47 25.36        23.83          24.40   \n",
       "sciq (acc)                 75.20 56.20        56.90          54.10   \n",
       "wsc273 (acc)               58.61 53.11        51.65          52.38   \n",
       "xnli (acc)                 35.01 34.32        34.22          34.10   \n",
       "\n",
       "                            mla0-96-192  mla0-0-192  mla0-128-0  mla192-0-0  \\\n",
       "hellaswag (acc)                   27.21       27.14       27.05       27.14   \n",
       "lambada_openai (acc)              11.62       15.02       13.89       15.82   \n",
       "mmlu (acc)                        22.92       22.92       22.94       22.95   \n",
       "piqa (acc)                        60.01       61.37       60.50       61.21   \n",
       "wikitext (word_perplexity)        92.87       82.44       85.11       81.84   \n",
       "winogrande (acc)                  52.57       51.46       50.59       49.96   \n",
       "commonsense_qa (acc)              19.57       19.57       19.57       19.57   \n",
       "pubmedqa (acc)                    34.40       39.20       35.00       37.60   \n",
       "race (acc)                        24.11       24.02       25.74       26.32   \n",
       "sciq (acc)                        51.90       57.50       55.30       57.90   \n",
       "wsc273 (acc)                      54.95       49.82       51.65       49.45   \n",
       "xnli (acc)                        34.67       34.47       34.70       34.43   \n",
       "\n",
       "                            mla0-0-0  \n",
       "hellaswag (acc)                27.11  \n",
       "lambada_openai (acc)           17.25  \n",
       "mmlu (acc)                     22.95  \n",
       "piqa (acc)                     61.32  \n",
       "wikitext (word_perplexity)     80.63  \n",
       "winogrande (acc)               53.28  \n",
       "commonsense_qa (acc)           19.57  \n",
       "pubmedqa (acc)                 38.20  \n",
       "race (acc)                     25.36  \n",
       "sciq (acc)                     56.20  \n",
       "wsc273 (acc)                   53.11  \n",
       "xnli (acc)                     34.32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# Full-tasks table (display-only, no saving)\n",
    "# =========================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# -------------------\n",
    "# Config / Constants\n",
    "# -------------------\n",
    "\n",
    "# Tasks to keep (rows will be task/metric pairs)\n",
    "TASKS = [\n",
    "    \"hellaswag\",\n",
    "    \"lambada_openai\",\n",
    "    \"mmlu\",\n",
    "    \"piqa\",\n",
    "    \"wikitext\",\n",
    "    \"winogrande\",\n",
    "    \"commonsense_qa\",\n",
    "    \"pubmedqa\",\n",
    "    \"race\",\n",
    "    \"sciq\",\n",
    "    \"wsc273\",\n",
    "    \"xnli\",\n",
    "]\n",
    "\n",
    "# Recognized model tokens to normalize model names (order here defines column order)\n",
    "MODEL_TOKENS = [\n",
    "    \"gpt2\",\n",
    "    \"mha\",\n",
    "    \"mla192-96-0\",\n",
    "    \"mla192-96-192\",\n",
    "    \"mla0-96-192\",\n",
    "    \"mla0-0-192\",\n",
    "    \"mla0-128-0\",\n",
    "    \"mla192-0-0\",\n",
    "    \"mla0-0-0\",\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Helpers\n",
    "# -------------------\n",
    "\n",
    "def load_wandb_export(csv_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Load the W&B CSV export.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def find_model_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Heuristically find the column that contains the model/run name including 'full-tasks'.\n",
    "    If several columns match, pick the one with the most matches.\n",
    "    \"\"\"\n",
    "    candidates = [c for c in df.columns if df[c].astype(str).str.contains(\"full-tasks\", case=False, na=False).any()]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    counts = {c: df[c].astype(str).str.contains(\"full-tasks\", case=False, na=False).sum() for c in candidates}\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "def normalize_model_name(text: Any) -> Optional[str]:\n",
    "    \"\"\"Map a raw model string to our canonical short token (longest match wins).\"\"\"\n",
    "    s = str(text).lower()\n",
    "    matches = [tok for tok in MODEL_TOKENS if tok in s]\n",
    "    if not matches:\n",
    "        return None\n",
    "    matches.sort(key=len, reverse=True)\n",
    "    return matches[0]\n",
    "\n",
    "def select_task_metric_columns(df: pd.DataFrame, tasks: List[str]) -> List[str]:\n",
    "    \"\"\"Return columns of the form 'task/metric' for the tasks we care about.\"\"\"\n",
    "    cols: List[str] = []\n",
    "    for col in df.columns:\n",
    "        if \"/\" not in col:\n",
    "            continue\n",
    "        task, metric = col.split(\"/\", 1)\n",
    "        if task in tasks:\n",
    "            cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def pretty_row_label(col_name: str) -> str:\n",
    "    \"\"\"Format 'task/metric' -> 'task\\\\n(metric)'.\"\"\"\n",
    "    task, metric = col_name.split(\"/\", 1)\n",
    "    return f\"{task} ({metric})\"\n",
    "\n",
    "import re\n",
    "\n",
    "def _parse_label(label: str):\n",
    "    \"\"\"\n",
    "    From 'task (metric)' return ('task', 'metric').\n",
    "    Falls back to ('label', '') if pattern not found.\n",
    "    \"\"\"\n",
    "    m = re.match(r\"^(.*?)\\s*\\(([^()]*)\\)\\s*$\", str(label))\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)\n",
    "    return str(label), \"\"\n",
    "\n",
    "def scale_acc_like(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Multiply acc/acc_norm rows by 100 and round to 2 d.p.\n",
    "    \"\"\"\n",
    "    task, metric = _parse_label(series.name)\n",
    "    if metric in {\"acc\", \"acc_norm\"}:\n",
    "        return pd.to_numeric(series, errors=\"coerce\").mul(100).round(2)\n",
    "    return series\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Core builder\n",
    "# -------------------\n",
    "\n",
    "def build_full_tasks_dataframe(csv_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the display DataFrame:\n",
    "      - Columns: model tokens (only those present among 'full-tasks' runs)\n",
    "      - Rows: 'task\\\\n(metric)' for selected TASKS\n",
    "      - 'acc' and 'acc_norm' are scaled to percentages (2 d.p.)\n",
    "    \"\"\"\n",
    "    df = load_wandb_export(csv_path)\n",
    "\n",
    "    # Identify column that contains 'full-tasks'\n",
    "    model_col = find_model_column(df)\n",
    "    if model_col is None:\n",
    "        raise RuntimeError(\"Could not find any column containing 'full-tasks' to identify model rows.\")\n",
    "\n",
    "    # Keep only rows with 'full-tasks'\n",
    "    ft_mask = df[model_col].astype(str).str.contains(\"full-tasks\", case=False, na=False)\n",
    "    df_ft = df[ft_mask].copy()\n",
    "    if df_ft.empty:\n",
    "        raise RuntimeError(\"No rows with 'full-tasks' found in the CSV.\")\n",
    "\n",
    "    # Normalize model names\n",
    "    df_ft[\"__model__\"] = df_ft[model_col].apply(normalize_model_name)\n",
    "    df_ft = df_ft.dropna(subset=[\"__model__\"])\n",
    "    if df_ft.empty:\n",
    "        raise RuntimeError(\"No recognizable model tokens among 'full-tasks' rows.\")\n",
    "\n",
    "    # Select 'task/metric' columns for tasks of interest\n",
    "    metric_cols = select_task_metric_columns(df_ft, TASKS)\n",
    "    if not metric_cols:\n",
    "        raise RuntimeError(\"No columns of the form 'task/metric' for the requested TASKS were found.\")\n",
    "\n",
    "    # Build per-model series (take the last occurrence if multiple rows per model)\n",
    "    per_model: dict[str, pd.Series] = {}\n",
    "    for model, g in df_ft.groupby(\"__model__\", sort=False):\n",
    "        row = g.iloc[-1]\n",
    "        ser = row[metric_cols]\n",
    "        ser.index = [pretty_row_label(c) for c in ser.index]  # task\\n(metric)\n",
    "        per_model[model] = ser\n",
    "\n",
    "    # Combine\n",
    "    out = pd.DataFrame(per_model)\n",
    "\n",
    "    # Scale acc/acc_norm rows to percentages\n",
    "    out = out.apply(scale_acc_like, axis=1)\n",
    "\n",
    "    # ... later, where you sort rows:\n",
    "    order_map = {t: i for i, t in enumerate(TASKS)}\n",
    "\n",
    "    def row_key(lbl: str):\n",
    "        task, metric = _parse_label(lbl)\n",
    "        return (order_map.get(task, 999), metric)\n",
    "    out = out.reindex(sorted(out.index, key=row_key))\n",
    "\n",
    "    # Order columns by MODEL_TOKENS, then any others\n",
    "    cols = [m for m in MODEL_TOKENS if m in out.columns] + [c for c in out.columns if c not in MODEL_TOKENS]\n",
    "    out = out[cols]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Example usage\n",
    "# -------------------\n",
    "# Set your CSV path (W&B export)\n",
    "csv_path = \"./wandb_export_2025-10-03T09_56_38.127-05_00.csv\"  # <- change if needed\n",
    "\n",
    "df_results = build_full_tasks_dataframe(csv_path)\n",
    "\n",
    "# Nicely display in the notebook (no saving to disk)\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd03235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
