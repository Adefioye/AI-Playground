{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's install lm-eval\n",
    "!git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "!cd lm-evaluation-harness\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb extra for lm-eval\n",
    "!pip install lm_eval[wandb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f75089",
   "metadata": {},
   "source": [
    "## Usage\n",
    "The goal here is to run evals on the following datasets\n",
    "```\n",
    "TASKS = [\n",
    "    \"wikitext\",         # Perplexity on WikiText\n",
    "    \"lambada_openai\",   # Cloze/Prediction task\n",
    "    \"hellaswag\",        # Commonsense NLI\n",
    "    \"piqa\",             # Physical Interaction QA\n",
    "    \"winogrande\",       # Commonsense Reasoning (Winograd Schema)\n",
    "    \"arc_easy\",         # AI2 Reasoning Challenge (Easy)\n",
    "    \"arc_challenge\",    # AI2 Reasoning Challenge (Challenge)\n",
    "    \"openbookqa\",       # Open Book Question Answering\n",
    "    \"mmlu\",             # Massive Multitask Language Understanding\n",
    "    \"gsm8k\",            # Grade School Math\n",
    "]\n",
    "```\n",
    "\n",
    "All datasets are to be evaluated using zero-shot except `mmlu` and `gsm8k`.\n",
    "\n",
    "`mmlu` should use `5 few-shot`\n",
    "\n",
    "`gsm8k` should use `8 few-shot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lm-eval to evaluate a model on HF on based on the instructions below\n",
    "# ```\n",
    "\n",
    "# All datasets are to be evaluated using zero-shot except `mmlu` and `gsm8k`.\n",
    "\n",
    "# `mmlu` should use `5 few-shot`\n",
    "\n",
    "# `gsm8k` should use `8 few-shot`\n",
    "\n",
    "# You can run the evaluation using the command below\n",
    "# TASKS = [\n",
    "#     \"wikitext\",         # Perplexity on WikiText\n",
    "#     \"lambada_openai\",   # Cloze/Prediction task\n",
    "#     \"hellaswag\",        # Commonsense NLI\n",
    "#     \"piqa\",             # Physical Interaction QA\n",
    "#     \"winogrande\",       # Commonsense Reasoning (Winograd Schema)\n",
    "#     \"arc_easy\",         # AI2 Reasoning Challenge (Easy)\n",
    "#     \"arc_challenge\",    # AI2 Reasoning Challenge (Challenge)\n",
    "#     \"openbookqa\",       # Open Book Question Answering\n",
    "#     \"mmlu\",             # Massive Multitask Language Understanding\n",
    "#     \"gsm8k\",            # Grade School Math\n",
    "# ]\n",
    "# ``\n",
    "\n",
    "# New tasks to test on\n",
    "# SciQ, RACE, ReCORD, SST, MRPC, RTE, MultiNLI, WSC273, WiC.\n",
    "NEW_TASKS = [\n",
    "    \"sciq\",\n",
    "    \"race\",\n",
    "    \"super_glue\",\n",
    "    \"swag\",\n",
    "    \"anli\",\n",
    "    \"xnli\",\n",
    "    \"wsc273\",\n",
    "    \"pubmedqa\",\n",
    "    \"mathqa\",\n",
    "    \"siqa\"\n",
    "]\n",
    "\n",
    "# Let's test things out with one model first\n",
    "model_checkpoints = [\n",
    "    \"kokolamba/SubspaceDecoder_mla192-96-0\",\n",
    "    \"kokolamba/SubspaceDecoder_mla192-96-192\",\n",
    "    \"kokolamba/SubspaceDecoder_mla0-128-0\",\n",
    "    \"kokolamba/SubspaceDecoder_mla0-96-192\",\n",
    "    \"kokolamba/SubspaceDecoder_mla0-0-192\",\n",
    "    \"kokolamba/SubspaceDecoder_mla0-0-0\",\n",
    "    \"kokolamba/SubspaceDecoder_mla192-0-0\",\n",
    "    \"kokolamba/SubspaceDecoder_mha\"\n",
    "]\n",
    "\n",
    "# !lm_eval --model hf \\\n",
    "#     --model_args pretrained=kokolamba/SubspaceDecoder_mha,trust_remote_code=True \\\n",
    "#     --tasks wikitext,lambada_openai,hellaswag,piqa,winograde,arc_easy,arc_challenge,openbookqa,mmlu,gsm8k_cot\\\n",
    "#     --device cuda:0 \\\n",
    "#     --batch_size auto:4 \\\n",
    "#     --output_path results \\\n",
    "#     --wandb_args project=subspace-decoder-lm-harness-results \\\n",
    "#     --log_samples \\\n",
    "#     --limit 10\n",
    "import subprocess\n",
    "\n",
    "# Pick one model checkpoint\n",
    "ckpt = \"kokolamba/SubspaceDecoder_mha\"\n",
    "\n",
    "print(f\"Running evaluation for {ckpt}...\")\n",
    "\n",
    "cmd = [\n",
    "    \"lm_eval\",\n",
    "    \"--model\", \"hf\",\n",
    "    \"--model_args\", f\"pretrained={ckpt},trust_remote_code=True\",\n",
    "    \"--tasks\", \"wikitext,lambada_openai,hellaswag,piqa,winograde,arc_easy,arc_challenge,openbookqa,mmlu,gsm8k_cot\",\n",
    "    \"--device\", \"cuda:0\",\n",
    "    \"--batch_size\", \"auto:4\",\n",
    "    \"--output_path\", \"results\",\n",
    "    \"--wandb_args\", \"project=subspace-decoder-lm-harness-results\",\n",
    "    \"--log_samples\",\n",
    "    \"--limit\", \"10\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subprocess to run the command for each model in the list\n",
    "import subprocess\n",
    "\n",
    "for ckpt in model_checkpoints:\n",
    "    print(f\"Running evaluation for {ckpt}...\")\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", f\"pretrained={ckpt},trust_remote_code=True\",\n",
    "        \"--tasks\", \"wikitext,lambada_openai,hellaswag,piqa,winograde,arc_easy,arc_challenge,openbookqa,mmlu,gsm8k_cot\",\n",
    "        \"--device\", \"cuda:0\",\n",
    "        \"--batch_size\", \"auto:4\",\n",
    "        \"--output_path\", \"results\",\n",
    "        \"--wandb_args\", \"project=subspace-decoder-lm-harness-results\",\n",
    "        \"--log_samples\",\n",
    "        \"--limit\", \"10\"\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
